# -*- coding: utf-8 -*-
"""TFandLab2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lOMdb64Uvt5oD-e3sniH62NZ6Ffv9t65
"""

import tensorflow as tf
from tensorflow import keras
import numpy as np

data = keras.datasets.fashion_mnist
(tx,ty) , (testx,testy) = data.load_data()

import pandas as pd

#get the train/test split package from skelearn for preparing our dataset to train and test the model with
from sklearn.model_selection import train_test_split

#Import the numpy library to work with and manipulate the data
import numpy as np

"""# New Section"""

dataset = pd.read_csv('/content/dataset.csv')

dataset = dataset.dropna()

print('first rows:')
dataset.head()

import matplotlib.pyplot as plt
columns = dataset.columns.drop(['y'])
rows = dataset.head(20)
fig, ax = plt.subplots(nrows=10, ncols=1)
data = range(0,rows.shape[0])
index=0
# Plot the feature subplots for given housing dataset
# Each feature will be having a seperate subplot
print('California Housing')
for column in columns:
  ax[index].plot(data, rows[column], label=column)
  ax[index].legend()
  index=index+1

"""# New Section"""

import sklearn


#will predict the median house value column
Y = dataset['median_house_val']

#selecting from longitude column to median column
X= dataset.loc[:,'longitude':'median_income']

#X = sklearn.preprocessing.normalize(X)

#spliting the dataset into training and testing
x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size = 0.3)

#converting the datasets into numpy arrays to work with our Pytorch model
x_train_np = x_train.to_numpy()
y_train_np = y_train.to_numpy()

#converting the testing datasets into numpy arrays
x_test_np = x_test.to_numpy()
y_test_np = y_test.to_numpy()

import torch

#import the 1D convolution layer since it is tabular data or sequence data
from torch.nn import Conv1d

#import the max pooling layer
from torch.nn import MaxPool1d

#import the flatten layer
from torch.nn import Flatten

#import the linear layer
from torch.nn import Linear

#import the relu activation function
from torch.nn.functional import relu

#import the relu activation function
from torch.nn import BatchNorm1d


#import the dataloader and tensordataset libraries from pytorch to work with out datasets
#dataloader will take tensor dataset as ip and out the batch size
from torch.utils.data import DataLoader, TensorDataset

# passing module will help us use the parameters
class CnnRegressor(torch.nn.Module):

  def __init__(self, batch_size,inputs,outputs):
    super(CnnRegressor,self).__init__()
    self.batch_size = batch_size
    self.inputs = inputs
    self.outputs = outputs

    self.input_layer = Conv1d(inputs,batch_size,1)
    self.batch_normalization = BatchNorm1d(batch_size)
    self.max_pooling_layer = MaxPool1d(1)

    self.conv_layer = Conv1d(batch_size,128,1)
    self.batch_normalization2 = BatchNorm1d(128)
    self.max_pooling_layer2 = MaxPool1d(1)


    self.flatten_layer = Flatten()
 
    #define a linear layer (ips,opts)
    self.linear_layer = Linear(128,64)

    #define the output layer
    self.output_layer = Linear(64,outputs)

  def feed(self,input):
    input = input.reshape((self.batch_size,self.inputs,1))

    output = relu(self.batch_normalization(self.input_layer(input)))

    output = self.max_pooling_layer(output)

    output = relu(self.batch_normalization2(self.conv_layer(output)))

    output = self.flatten_layer(output)

    output = self.linear_layer(output)

    output = self.output_layer(output)
    return output

from torch.optim import  SGD

from torch.nn import L1Loss

!pip install pytorch.ignite
from ignite.contrib.metrics.regression.r2_score import R2Score

batch_size = 64

model = CnnRegressor(batch_size,X.shape[1],1)

model.cuda()

def model_loss(model,dataset,train = False, optimizer = None):
  performance = L1Loss()
  score_metric = R2Score()
  avg_loss = 0
  avg_score = 0
  count = 0

  for input,output in iter(dataset):
    predictions = model.feed(input)

    loss = performance(predictions,output)

    score_metric.update([predictions,output])
    score = score_metric.compute()

    if(train):
      #clear any errors so they dont cummulate
      optimizer.zero_grad()

      loss.backward()

      #use the optimizer to update the models parameters based on the gradients
      optimizer.step()

    avg_loss += loss.item()
    avg_score += score
    count += 1

  return avg_loss/count, avg_score/count

epochs = 500

optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)

inputs = torch.from_numpy(x_train_np).cuda().float()
outputs = torch.from_numpy(y_train_np.reshape(y_train_np.shape[0], 1)).cuda().float()

tensor = TensorDataset(inputs,outputs)
loader = DataLoader(tensor, batch_size, shuffle=True, drop_last=True)

for epoch in range(epochs):
  avg_loss, avg_r2_score = model_loss(model, loader, train=True, optimizer=optimizer)

  print("Epoch " + str(epoch+1) + ":\n\tLoss = " + str(avg_loss) + "\n\tR2 score = " + str(avg_r2_score))

inputs = torch.from_numpy(x_test_np).cuda().float()
outputs = torch.from_numpy(y_test_np.reshape(y_test_np.shape[0], 1)).cuda().float()

tensor = TensorDataset(inputs,outputs)
loader = DataLoader(tensor, batch_size, shuffle=True, drop_last=True)

#output the average performance of the model
avg_loss, avg_r2_score = model_loss(model,loader)
print("the model's L1 loss is " + str(avg_loss))
print("the model's R2 score is " + str(avg_r2_score))

trainedmodel = {'model': CnnRegressor(64,8,1),
          'state_dict': model.state_dict(),
          'optimizer' : optimizer.state_dict()}
torch.save(trainedmodel, '1102484_1dconv_reg')

"""# New Section"""